// nextflow.config

// ====================================================================================
//                            PIPELINE PARAMETERS
// ====================================================================================
// 


params {
    // =========================== Modification Area =================================
    // 1. Replace this with your conda env
    conda_base_path = '/path/to/base_env'  // First activate the bio_pipeline_env, then run "conda info" in the command line and check the "base environment" path.
    conda_env_path = '/path/to/bio_pipeline_env'      // First activate the bio_pipeline_env, then run "conda info" in the command line and check the "active env location" path.
    
    // 2. Root directory for other required software and scripts
    software_base_dir = '/path/to/required_software'
    haplocheckCLI      = "${params.software_base_dir}/haplocheckCLI-master/haplocheckCLI.jar"
    cromwell_jar       = "${params.software_base_dir}/cromwell-90.jar"
    
    // 3. Root directory for reference genomes, databases, and other required input files
    required_base_dir      = '/path/to/root_dir_for_all_required_files'

    // 4. Set key parameters 
    vaf_filter_threshold = 0.01   // Set the VAF filter threshold

    // Set the threhold for sample-level filter
    min_mt_cov = 100
    min_mtcn   = 50
    max_mtcn   = 500
    max_contam = 0.02

    // ================================================================================

    // The launch_pipeline.sh script will provide a path if PIPELINE_MODE is set to "disease".
    pipeline_mode = null
    disease_meta_file = null

     // These will be passed dynamically from launch_pipeline.sh
    input              = null
    outdir             = null

    // Define default paths consistent with the repo structure
    wdl_script   = "${baseDir}/MitochondriaPipeline_multisample.wdl"
    hail_script  = "${baseDir}/hail"
    picard = "${params.conda_env_path}/share/picard-slim-3.1.1-0/picard.jar"   //picard has been installed under conda env
    gatk   = "${params.conda_env_path}/share/gatk4-4.5.0.0-0/gatk-package-4.5.0.0-local.jar"   //picard has been installed under conda env

    // Interval list used for calculating nuclear coverage 
    wgs_intervals = "${params.required_base_dir}/required_files/autosome_XY.interval_list"
}

// ====================================================================================
//                       WDL and Hail INPUT PARAMETERS
// ====================================================================================

// --- Static inputs for the WDL workflow ---
// All paths required by your WDL script, except for the sample list, should be defined here.
// Please replace the placeholder paths with the actual paths on your system.
params.wdl_inputs = [

    vaf_filter_threshold: params.vaf_filter_threshold,

    // Software paths
    picard: params.picard,
    gatk:   params.gatk,
    haplocheckCLI:  params.haplocheckCLI,   // Note: This one is still manual

    compress_output_vcf: true,

    // Reference genome paths
    ref_fasta:       "${params.required_base_dir}/required_files/Homo_sapiens_assembly38.fasta",
    ref_dict:        "${params.required_base_dir}/required_files/Homo_sapiens_assembly38.dict",
    ref_fasta_index: "${params.required_base_dir}/required_files/Homo_sapiens_assembly38.fasta.fai",
    mt_dict:         "${params.required_base_dir}/required_files/Homo_sapiens_assembly38.chrM.dict",
    mt_fasta:        "${params.required_base_dir}/required_files/Homo_sapiens_assembly38.chrM.fasta",
    mt_fasta_index:  "${params.required_base_dir}/required_files/Homo_sapiens_assembly38.chrM.fasta.fai",
    mt_amb:          "${params.required_base_dir}/required_files/Homo_sapiens_assembly38.chrM.fasta.amb",
    mt_ann:          "${params.required_base_dir}/required_files/Homo_sapiens_assembly38.chrM.fasta.ann",
    mt_bwt:          "${params.required_base_dir}/required_files/Homo_sapiens_assembly38.chrM.fasta.bwt",
    mt_pac:          "${params.required_base_dir}/required_files/Homo_sapiens_assembly38.chrM.fasta.pac",
    mt_sa:           "${params.required_base_dir}/required_files/Homo_sapiens_assembly38.chrM.fasta.sa",
    mt_shifted_dict: "${params.required_base_dir}/required_files/Homo_sapiens_assembly38.chrM.shifted_by_8000_bases.dict",
    mt_shifted_fasta: "${params.required_base_dir}/required_files/Homo_sapiens_assembly38.chrM.shifted_by_8000_bases.fasta",
    mt_shifted_fasta_index: "${params.required_base_dir}/required_files/Homo_sapiens_assembly38.chrM.shifted_by_8000_bases.fasta.fai",
    mt_shifted_amb:   "${params.required_base_dir}/required_files/Homo_sapiens_assembly38.chrM.shifted_by_8000_bases.fasta.amb",
    mt_shifted_ann:  "${params.required_base_dir}/required_files/Homo_sapiens_assembly38.chrM.shifted_by_8000_bases.fasta.ann",
    mt_shifted_bwt:  "${params.required_base_dir}/required_files/Homo_sapiens_assembly38.chrM.shifted_by_8000_bases.fasta.bwt",
    mt_shifted_pac:  "${params.required_base_dir}/required_files/Homo_sapiens_assembly38.chrM.shifted_by_8000_bases.fasta.pac",
    mt_shifted_sa:   "${params.required_base_dir}/required_files/Homo_sapiens_assembly38.chrM.shifted_by_8000_bases.fasta.sa",

    // Other required files
    shift_back_chain: "${params.required_base_dir}/required_files/ShiftBack.chain",
    blacklisted_sites: "${params.required_base_dir}/required_files/blacklist_sites.hg38.chrM.bed",
    blacklisted_sites_index: "${params.required_base_dir}/required_files/blacklist_sites.hg38.chrM.bed.idx",
    non_control_region_interval_list: "${params.required_base_dir}/required_files/non_control_region.interval_list",
    control_region_shifted_reference_interval_list: "${params.required_base_dir}/required_files/control_region_shifted.interval_list"
]

params.hail_pipeline_config = [

        num_workers: 1,  

        // --- Input directories (will be created by the Nextflow process) ---
        // These are relative to the process work directory
        raw_vcf_dir: "wdl_outputs/vcfs",
        haplocheck_dir: "wdl_outputs/contamination",
        wgs_metrics_dir: "wdl_outputs/wgs_metrics",
        mt_coverage_dir: "wdl_outputs/coverage",
        
        // --- VEP cache directory ---
        vep_cache_dir: "${params.required_base_dir}/annotation_databases",
        reference_genome_path: "${params.required_base_dir}/Homo_sapiens_assembly38.fasta",

        // --- Output directories (will be created by the Nextflow process) ---
        vep_vcf_dir: "annotation/vep_vcf/",
        metadata_dir: "annotation/metadata/",
        fullhaplogroups: "annotation/metadata/haplogroup_full.txt",
        contamination: "annotation/metadata/contamination.txt",
        final_output_dir: "annotation/final_outputs/",

        // --- Input metadata files (please update these paths) ---
        disease_meta_file: params.disease_meta_file,
        
        // --- Annotation Database Paths (please update these paths) ---
        gnomadcache: "${params.required_base_dir}/annotation_databases/gnomad.genomes.v3.1.sites.chrM.reduced_annotations.tsv",
        clinvarcache: "${params.required_base_dir}/annotation_databases/clinvar_chrMT_02122024.txt",
        mitomap_polycache: "${params.required_base_dir}/annotation_databases/MITOMAP_polymorphisms_02122024.txt",
        mitomap_diseasecache: "${params.required_base_dir}/annotation_databases/MITOMAP_disease_02122024.txt",
        helixcache: "${params.required_base_dir}/annotation_databases/HelixMTdb_20200327.tsv",
        haplogroup_varcache: "${params.required_base_dir}/annotation_databases/rCRS-centered_vars_with_haplo_final_update.txt",
        mitimpactcache: "${params.required_base_dir}/annotation_databases/MitImpact_db_3.1.2.txt",
        mitotipcache: "${params.required_base_dir}/annotation_databases/mitotip_scores.txt",
        hmtvarcache: "${params.required_base_dir}/annotation_databases/hmtvar_annotations.txt"
    ]

// ====================================================================================
//                            EXECUTION PROFILES
// ====================================================================================

// =========================== Modification Area =================================
// These parameters control how Cromwell submits its own jobs to Slurm.
params.cromwell_options = [
    queue: '',
    runtime_minutes: 1440,
    cpus: 8,
    memory: 32768
]
cromwell_submit_rate_limit = 180

process {

    // --- Per-process resource specifications ---
    // (These are specific to main.nf and will be ignored by merge.nf)
    withName: 'ALIGN_AND_UNSORT' { cpus = 16; memory = '18.GB'; time = '12h' }
    withName: 'SORT_AND_CONVERT_TO_CRAM' { cpus = 16; memory = '18.GB'; time = '12h' }
    withName: 'MERGE_CRAMS' { cpus = 16; memory = '18.GB'; time = '12h' }
    withName: 'CONVERT_BAM_TO_CRAM' { cpus = 16; memory = '18.GB'; time = '12h' }
    withName: 'GENERATE_CRAM_TSV' { cpus = 1; memory = '1.GB'; time = '1h' }
    withName: 'GENERATE_WDL_JSON' { cpus = 1; memory = '1.GB'; time = '1h' }
    withName: 'RUN_WDL_VARIANT_CALLING' { cpus = 4; memory = '8.GB'; time = '12h' }
    withName: 'CALCULATE_MTCN' { cpus = 4; memory = '8.GB'; time = '12h' }
    withName: 'SAMPLE_LEVEL_FILTER' { cpus = 4; memory = '8.GB'; time = '12h' }
    withName: 'ANNOTATE_INDIVIDUAL_VCF' { cpus = 8; memory = '32.GB'; time = '12h' }

}

profiles {

    // --- Cluster profile (for Slurm) ---
    cluster {
        process {
            executor = 'slurm'
            // Default resources for processes not matching a 'withName'
            // (e.g., all processes in merge.nf)

            // This 'beforeScript' OVERRIDES the global one
            // for the 'cluster' profile only.
            beforeScript = """
                echo "INFO: Activating Conda environment: bio_pipeline_env"
                source ${params.conda_base_path}/etc/profile.d/conda.sh
                conda activate ${params.conda_env_path}

                export JAVA_HOME=\$(dirname \$(dirname \$(which java)))
                export PATH=\$JAVA_HOME/bin:\$PATH

                echo "JAVA CHECK >>>"
                which java
                java -version
                echo "JAVA_HOME: \$JAVA_HOME"
                echo "--- Sanity Check Completed ---"
            """
        }
        
        // Tell Cromwell to also use Slurm for its sub-jobs
        cromwell.backend = 'Slurm'
    }

}
// ====================================================================================
//                       EXECUTOR & PROCESS CONFIGURATION
// ====================================================================================

// This block controls the job submission rate to avoid overwhelming the scheduler
executor {
    name = 'slurm'
    queueSize = 20
    submitRateLimit = '180/h'
}

// Enable DSL2, the modern Nextflow syntax
nextflow.enable.dsl=2
